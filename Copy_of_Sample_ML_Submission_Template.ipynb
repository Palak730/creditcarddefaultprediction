{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palak730/creditcarddefaultprediction/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Credit Card Default Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  Palak Srivastava\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this study is to exploit some supervised machine learning algorithms to identify the key drivers that determine the likelihood of credit card default, underlining the mathematical aspects behind the methods used. Credit card default happens when you have become severely delinquent on your credit card payments. In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, the overused credit card for consumption and accumulated heavy credit and debts\n",
        "\n",
        "The goal is to build an automated model for both identifying the key factors, and predicting a credit card default based on the information about the client and historical transactions. The general concepts of the supervised machine learning paradigm are later reported, together with a detailed explanation of all techniques and algorithms used to build the models. In particular, Logistic Regression, Random Forest and Support Vector Machines algorithms have been applied."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WxzveD_lG1yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O, data manipulation\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "from pandas import set_option\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import KFold # for cross validation\n",
        "from sklearn.model_selection import GridSearchCV # for tuning parameter\n",
        "from sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\n",
        "from sklearn.preprocessing import StandardScaler # for normalization\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import metrics # for the check the error and accuracy of the model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tet-QdxmJoA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/'\n",
        "data=pd.read_csv(file_path + '/creditcardclients.csv')"
      ],
      "metadata": {
        "id": "tS0lnxDDJfsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert integer columns to float\n",
        "float_columns = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5',\n",
        "                 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4',\n",
        "                 'PAY_AMT5', 'PAY_AMT6', 'LIMIT_BAL']\n",
        "data[float_columns] = data[float_columns].astype(float)\n"
      ],
      "metadata": {
        "id": "ypLqKPW1EN3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Dataset Information**"
      ],
      "metadata": {
        "id": "jFnXEfq7HOF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we know about dataset :\n",
        "\n",
        "We have records of 30000 customers. Below are the description of all features we have.\n",
        "\n",
        "ID: ID of each client\n",
        "\n",
        "LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
        "\n",
        "SEX: Gender (1 = male, 2 = female)\n",
        "\n",
        "EDUCATION: (1 = graduate school, 2 = university, 3 = high school, 0,4,5,6 = others)\n",
        "\n",
        "MARRIAGE: Marital status (0 = others, 1 = married, 2 = single, 3 = others)\n",
        "\n",
        "AGE: Age in years\n",
        "Scale for PAY_0 to PAY_6 : (-2 = No consumption, -1 = paid in full, 0 = use of revolving credit (paid minimum only), 1 = payment delay for one month, 2 = payment delay for two months, ... 8 = payment delay for eight months, 9 = payment delay for nine months and above)\n",
        "\n",
        "PAY_0: Repayment status in September, 2005 (scale same as above)\n",
        "\n",
        "PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "\n",
        "PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "\n",
        "PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "\n",
        "PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "\n",
        "PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "\n",
        "BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "\n",
        "BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "\n",
        "BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "\n",
        "BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "\n",
        "BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "\n",
        "BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "\n",
        "PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "\n",
        "default.payment.next.month: Default payment (1=yes, 0=no)\n",
        "In our dataset we got customer credit card transaction history for past 6 month , on basis of which we have to predict if customer will default or not.\n",
        "\n",
        "So let's begin."
      ],
      "metadata": {
        "id": "hUSqeWAhksyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "GiUffpzRHV_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "z5mwUVjQHebm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will check if we have any null values"
      ],
      "metadata": {
        "id": "NEzjD3LqHvi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "9caanIA-HxMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Data Preprocessing**\n",
        "\n",
        "**Changing name of some columns for simplicity and better understanding**"
      ],
      "metadata": {
        "id": "tN1fH_IJwqC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming columns\n",
        "data.rename(columns = {'default payment next month' : 'Defaulter'  },inplace=True )\n",
        "data.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "data.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "data.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ],
      "metadata": {
        "id": "9d6aoFE-p5ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'},\n",
        "               'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'others'},\n",
        "               'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)"
      ],
      "metadata": {
        "id": "M2rH8rlVJrN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "zqh_rqxfwhSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Exploratory Data Analysis***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculate the frequency of defaults and non-defaults credit cards**"
      ],
      "metadata": {
        "id": "iEEIHHE7BjuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequency of defaults and non-defaults\n",
        "yes = data['Defaulter'].sum()\n",
        "no = len(data) - yes\n",
        "\n",
        "# Calculate the percentage of defaults and non-defaults\n",
        "total = len(data)\n",
        "yes_perc = round(yes / total * 100, 1)\n",
        "no_perc = round(no / total * 100, 1)\n",
        "\n",
        "# Set the figure size and context\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.set_context('notebook', font_scale=1.2)\n",
        "\n",
        "# Create the count plot\n",
        "sns.countplot(x='Defaulter', data=data, palette=\"Blues\")\n",
        "\n",
        "# Annotate the counts and percentages\n",
        "plt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\n",
        "plt.annotate('Default: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\n",
        "plt.annotate(str(no_perc) + \"%\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\n",
        "plt.annotate(str(yes_perc) + \"%\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title('COUNT OF CREDIT CARDS', size=14)\n",
        "plt.xlabel('Default')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Remove the top and right spines\n",
        "sns.despine()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bTB94SqvBj5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this sample of 30,000 credit card holders, there were 6,636 default credit cards; that is, the proportion of default in the data is 22,1%.         "
      ],
      "metadata": {
        "id": "S4P2RRGZBkId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**SEX**\n"
      ],
      "metadata": {
        "id": "c1XSlG8whETj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['SEX'].value_counts()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Education**\n"
      ],
      "metadata": {
        "id": "fzipeis2QKK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the values 5, 6, and 0 in the 'EDUCATION' column and replace them with 'others'\n",
        "condition = (data['EDUCATION'] == 5) | (data['EDUCATION'] == 6) | (data['EDUCATION'] == 0) | (data['EDUCATION'] == 4)\n",
        "data.loc[condition, 'EDUCATION'] = 'others'\n",
        "\n",
        "# Print the updated value counts for each category in the 'EDUCATION' column\n",
        "print(data['EDUCATION'].value_counts())"
      ],
      "metadata": {
        "id": "AmN9OCnNL6T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For values without descriptions (5, 6, 0), we can group them as \"Others\" and represent them as 4 in the dataset."
      ],
      "metadata": {
        "id": "xJBaAW9JtR0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Marriage**\n",
        "\n"
      ],
      "metadata": {
        "id": "ljC-VzJ7uWbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the value 0 in the 'MARRIAGE' column and replace them with others using .loc\n",
        "condition = (data['MARRIAGE'] == 0) | (data['MARRIAGE'] == 3)\n",
        "data.loc[condition, 'MARRIAGE'] = 'others'\n",
        "data['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "CVQ9tCPLueqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have few values for 0, which are not determined . So I am adding them in Others category."
      ],
      "metadata": {
        "id": "TpJT6snMusFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting our categorical features**"
      ],
      "metadata": {
        "id": "Rzapo2AYvOvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']"
      ],
      "metadata": {
        "id": "slrbPhk1vO4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame 'df_cat' containing only the categorical features\n",
        "df_cat = data[categorical_features].copy()\n",
        "\n",
        "# Use .loc to set the 'Defaulter' column in 'df_cat' based on the 'Defaulter' column in the original 'data' DataFrame\n",
        "df_cat.loc[:, 'Defaulter'] = data['Defaulter']"
      ],
      "metadata": {
        "id": "vYvN8HyhvPK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_features:\n",
        "  plt.figure(figsize=(8,5))\n",
        "  fig, axes = plt.subplots(ncols=2,figsize=(13,8))\n",
        "  data[col].value_counts().plot(kind=\"pie\",ax = axes[0],subplots=True)\n",
        "  sns.countplot(x = col, hue = 'Defaulter', data = df_cat)"
      ],
      "metadata": {
        "id": "NalsxK5ovgWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations for categorical features:\n",
        "\n",
        "Gender: The majority of credit card holders who defaulted are females, indicating a higher proportion of female defaulters.\n",
        "\n",
        "Education: Defaulters are more likely to be educated, with a higher proportion having completed graduate school or university education.\n",
        "\n",
        "Marital Status: Defaulters are more likely to be singles, suggesting a higher proportion of single individuals among the group of credit card holders who defaulted."
      ],
      "metadata": {
        "id": "eGaZpX18wjFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limit Balance"
      ],
      "metadata": {
        "id": "jVzk-aHDwuwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['LIMIT_BAL'].max()\n"
      ],
      "metadata": {
        "id": "VSkpQf-jwyb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['LIMIT_BAL'].min()"
      ],
      "metadata": {
        "id": "Wf-EINqnwyu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['LIMIT_BAL'].describe()"
      ],
      "metadata": {
        "id": "aLzGndkixeI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='Defaulter', y='LIMIT_BAL', data=data)"
      ],
      "metadata": {
        "id": "lpz-ESzgxwDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the numerical features using histograms and box plots\n",
        "numerical_features = ['LIMIT_BAL', 'AGE']\n",
        "\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(data[feature], bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'{feature} Distribution')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x='Defaulter', y=feature, data=data, palette='pastel')\n",
        "    plt.xlabel('Default')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} by Defaulters')\n",
        "    plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6pPBvcEWnMeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The 'LIMIT_BAL' histogram shows that a significant number of credit card holders have lower credit limits, while the 'AGE' histogram suggests a relatively uniform distribution across various age groups.\n",
        "\n",
        "2. The box plots indicate that the median credit limit for customers who defaulted is slightly lower than those who did not default, but there are no significant differences in the median ages between defaulters and non-defaulters."
      ],
      "metadata": {
        "id": "UZBJW3cppkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AGE**\n",
        "\n"
      ],
      "metadata": {
        "id": "ojdJuiLsqckl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['AGE'].value_counts()\n",
        "data['AGE']=data['AGE'].astype('int')\n"
      ],
      "metadata": {
        "id": "vlwl2BYIp50_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot for the age distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "age_counts_df = data['AGE'].value_counts().reset_index()\n",
        "sns.barplot(x='index', y='AGE', data=age_counts_df, palette='pastel')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Age Distribution - Bar Plot')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "#values count for Age with respect to IsDefaulter\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.countplot(x = 'AGE', hue = 'Defaulter', data = data)"
      ],
      "metadata": {
        "id": "Q6kr4kzvrmqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BILL AMOUNT**\n",
        "\n",
        "Creating a pair plot to visualize the pairwise relationships and distributions among the bill amount columns from the original dataset."
      ],
      "metadata": {
        "id": "MDuPw5citGm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the bill amount columns\n",
        "bill_amnt_df = data[['BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR']]\n",
        "\n",
        "# Create a pair plot for the bill amount columns\n",
        "sns.pairplot(data=bill_amnt_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jL--90sltG6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**History payment status**\n",
        "\n",
        "The count plots visualize the distribution of payment status for defaulters and non-defaulters in different months, providing insights into how payment behavior relates to the likelihood of defaulting."
      ],
      "metadata": {
        "id": "C_i2530-zZZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the payment status columns\n",
        "pay_col = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\n",
        "\n",
        "# Create count plots for the payment status columns\n",
        "for col in pay_col:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(x=col, hue='Defaulter', data=data, palette='dark')\n",
        "    plt.xlabel(f'{col} (Repayment Status)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Payment Status Distribution - {col}')\n",
        "    plt.legend(title='Defaulter', labels=['No', 'Yes'])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "tYisLkXMttqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and visualize correlations between features\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k3r3os6Oo3ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Label Incoding**"
      ],
      "metadata": {
        "id": "wiAorXr9FDcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.replace({'SEX':{'FEMALE': 0, 'MALE' : 1},\n",
        "               'EDUCATION' : { 'graduate school' : 1 ,  'university' :2 , 'high school' : 3,  'others' : 4},\n",
        "               'MARRIAGE' : { 'married' : 1,  'single':2,  'others' : 3}}, inplace = True)"
      ],
      "metadata": {
        "id": "L8nxWS8AHVA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "h4WJkt8tboUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One Hot Encoding**"
      ],
      "metadata": {
        "id": "kOtx-VyGFXwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding for the 'EDUCATION' and 'MARRIAGE' columns\n",
        "data = pd.get_dummies(data, columns=[\"EDUCATION\", \"MARRIAGE\"])"
      ],
      "metadata": {
        "id": "E-xSz0SnFKhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "mGnMsDieFKk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['EDUCATION_4', 'MARRIAGE_3'], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "NoCuXZv5FKsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variables by droping firs variable\n",
        "data = pd.get_dummies(data, columns = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR'], drop_first = True )"
      ],
      "metadata": {
        "id": "Y_jb3ZExF4T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "4PB6uUJwF4Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using SMOTE (Synthetic Minority Oversampling Technique) to remediate class imbalance is a common approach in machine learning when dealing with imbalanced datasets. SMOTE is a technique that generates synthetic samples for the minority class by interpolating between existing samples."
      ],
      "metadata": {
        "id": "OE95I_FE-p-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your DataFrame named 'data' with features and the target column 'Defaulter'\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "x_smote, y_smote = smote.fit_resample(data[(i for i in list(data.describe(include='all').columns) if i != 'Defaulter')], data['Defaulter'])\n",
        "\n",
        "print('Original unbalanced dataset shape', len(data))\n",
        "print('Resampled balanced dataset shape', len(y_smote))\n"
      ],
      "metadata": {
        "id": "1XAoOGX9-0AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the balanced DataFrame 'balanced_data' from the resampled data\n",
        "balanced_data = pd.DataFrame(x_smote, columns=[col for col in data.columns if col != 'Defaulter'])\n",
        "# Add the 'Defaulter' column to the balanced DataFrame\n",
        "balanced_data['Defaulter'] = y_smote\n",
        "\n",
        "# Print the shape of the balanced DataFrame\n",
        "print(balanced_data.shape)"
      ],
      "metadata": {
        "id": "2QUF-obRF8cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing feature ID from dataset\n",
        "balanced_data.drop('ID',axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "OmRKbNVrcGIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_data.head()"
      ],
      "metadata": {
        "id": "jxgFIvNtcOzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seperating dependant and independant variabales\n",
        "X = balanced_data[(list(i for i in list(balanced_data.describe(include='all').columns) if i != 'Defaulter'))]\n",
        "y = balanced_data['Defaulter']"
      ],
      "metadata": {
        "id": "cDT5wwhRcdAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "B_5lmCUqc6Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "SeM6f3EGc6I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation"
      ],
      "metadata": {
        "id": "mWRcyZrpesti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "QyzAyR0V-htF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Splitting"
      ],
      "metadata": {
        "id": "FTKu0GMUfRLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AwS1uDRI-hwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape\n"
      ],
      "metadata": {
        "id": "eEKWf-l--h11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "fezQUQgD-h52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models used for predictions:**\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Decision Tree\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "SVM\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBjQOKIGDjLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest Classifier**"
      ],
      "metadata": {
        "id": "NzyQnFe3Ss74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the Random Forest Classifier model\n",
        "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y on test data\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "y_train_pred_rf = rf_classifier.predict(X_train)\n",
        "\n",
        "\n",
        "# Getting all scores for Random Forest Classifier on test data\n",
        "train_accuracy_rf = accuracy_score( y_train_pred_rf , y_train)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_score_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_score_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_score_rf = f1_score(y_test, y_pred_rf)\n",
        "roc_score_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"The accuracy on train data is:\", round(train_accuracy_rf, 3))\n",
        "print(\"The accuracy on test data is:\", round(accuracy_rf, 3))\n",
        "print(\"The precision on test data is:\", round(precision_score_rf, 3))\n",
        "print(\"The recall on test data is:\", round(recall_score_rf, 3))\n",
        "print(\"The f1 on test data is:\", round(f1_score_rf, 3))\n",
        "print(\"The roc_score on test data is:\", round(roc_score_rf, 3))\n"
      ],
      "metadata": {
        "id": "vd7UZZ0GAdKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "cm_rf_test = confusion_matrix(y_test, y_pred_rf)\n",
        "# Example to print confusion matrix for Logistic Regression\n",
        "print(\"\\nConfusion Matrix for Random Forest - Test:\")\n",
        "print(cm_rf_test)"
      ],
      "metadata": {
        "id": "ABMqNGhM1D71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "gRpXR231S0gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the Decision Tree Classifier model\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y on test data\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "y_train_pred_dt = dt_classifier.predict(X_train)\n",
        "\n",
        "# Getting all scores for Decision Tree Classifier on test data\n",
        "train_accuracy_dt = accuracy_score(y_train_pred_dt, y_train)\n",
        "accuracy_dt = accuracy_score(y_pred_dt, y_test)\n",
        "precision_score_dt = precision_score(y_pred_dt, y_test)\n",
        "recall_score_dt = recall_score(y_pred_dt, y_test)\n",
        "f1_score_dt = f1_score(y_pred_dt, y_test)\n",
        "roc_score_dt = roc_auc_score(y_pred_dt, y_test)\n",
        "\n",
        "print(\"The accuracy on train data is:\", round(train_accuracy_dt, 3))\n",
        "print(\"The accuracy on test data is:\", round(accuracy_dt, 3))\n",
        "print(\"The precision on test data is:\", round(precision_score_dt, 3))\n",
        "print(\"The recall on test data is:\", round(recall_score_dt, 3))\n",
        "print(\"The f1 on test data is:\", round(f1_score_dt, 3))\n",
        "print(\"The roc_score on test data is:\", round(roc_score_dt, 3))\n",
        "\n"
      ],
      "metadata": {
        "id": "ggah3pQjAdHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "cm_dt_test = confusion_matrix(y_test, y_pred_dt)\n",
        "print(\"\\nConfusion Matrix for Decision Tree - Test:\")\n",
        "print(cm_dt_test)"
      ],
      "metadata": {
        "id": "i8jsE3LLAdSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression**"
      ],
      "metadata": {
        "id": "oE_FS7NKS_zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the Logistic Regression model\n",
        "logi = LogisticRegression()\n",
        "logi.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y\n",
        "y_pred_logi = logi.predict(X_test)\n",
        "y_train_pred_logi = logi.predict(X_train)\n",
        "\n",
        "# Getting all scores for Logistic Regression\n",
        "train_accuracy_logi = round(accuracy_score(y_train, y_train_pred_logi), 3)\n",
        "accuracy_logi = round(accuracy_score(y_test, y_pred_logi), 3)\n",
        "precision_score_logi = round(precision_score(y_test, y_pred_logi), 3)\n",
        "recall_score_logi = round(recall_score(y_test, y_pred_logi), 3)\n",
        "f1_score_logi = round(f1_score(y_test, y_pred_logi), 3)\n",
        "roc_score_logi = round(roc_auc_score(y_test, y_pred_logi), 3)\n",
        "\n",
        "print(\"The accuracy on train data is:\", train_accuracy_logi)\n",
        "print(\"The accuracy on test data is:\", accuracy_logi)\n",
        "print(\"The precision on test data is:\", precision_score_logi)\n",
        "print(\"The recall on test data is:\", recall_score_logi)\n",
        "print(\"The f1 on test data is:\", f1_score_logi)\n",
        "print(\"The roc_score on test data is: \", roc_score_logi)\n"
      ],
      "metadata": {
        "id": "mM04v6e7AdVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "cm_logi_test = confusion_matrix(y_test, y_pred_logi)\n",
        "#print confusion matrix for Logistic Regression\n",
        "print(\"\\nConfusion Matrix for Logistic Regression - Test:\")\n",
        "print(cm_logi_test)"
      ],
      "metadata": {
        "id": "qfLoWAhMAdP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting**"
      ],
      "metadata": {
        "id": "F-dGAIhUT0vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the Gradient Boosting Classifier model\n",
        "gb_classifier = GradientBoostingClassifier( random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y on test data\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "y_train_pred_gb = gb_classifier.predict(X_train)\n",
        "\n",
        "# Getting all scores for Gradient Boosting Classifier on test data\n",
        "train_accuracy_gb = accuracy_score(y_train, y_train_pred_gb)\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "precision_score_gb = precision_score(y_test, y_pred_gb)\n",
        "recall_score_gb = recall_score( y_pred_gb, y_test)\n",
        "f1_score_gb = f1_score(y_test, y_pred_gb)\n",
        "roc_score_gb = roc_auc_score(y_test, y_pred_gb)\n",
        "\n",
        "print(\"The accuracy on train data is:\", round(train_accuracy_gb, 3))\n",
        "print(\"The accuracy on test data is:\", round(accuracy_gb, 3))\n",
        "print(\"The precision on test data is:\", round(precision_score_gb, 3))\n",
        "print(\"The recall on test data is:\", round(recall_score_gb, 3))\n",
        "print(\"The f1 on test data is:\", round(f1_score_gb, 3))\n",
        "print(\"The roc_score on test data is:\", round(roc_score_gb, 3))\n"
      ],
      "metadata": {
        "id": "o4zWQ0cEAdam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting\n",
        "cm_gb_test = confusion_matrix(y_test, y_pred_gb)\n",
        "print(\"\\nConfusion Matrix for Gradient Boosting - Test:\")\n",
        "print(cm_gb_test)\n"
      ],
      "metadata": {
        "id": "hZLyM8uxAddw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XGBoost Classifier**"
      ],
      "metadata": {
        "id": "1h6OqNh5WEWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and predict using XGBoost\n",
        "xgb_classifier = XGBClassifier(random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y on test data\n",
        "y_pred_xgb = xgb_classifier.predict(X_test)\n",
        "y_train_pred_xgb = xgb_classifier.predict(X_train) # Predict on train data\n",
        "\n",
        "# Getting all scores for  XBoosting Classifier on test data\n",
        "train_accuracy_xgb = accuracy_score( y_train_pred_xgb, y_train)\n",
        "accuracy_xgb = accuracy_score( y_pred_xgb , y_test)\n",
        "precision_score_xgb = precision_score(y_pred_xgb , y_test)\n",
        "recall_score_xgb = recall_score(y_pred_xgb , y_test)\n",
        "f1_score_xgb = f1_score(y_pred_xgb , y_test)\n",
        "roc_score_xgb = roc_auc_score(y_pred_xgb , y_test)\n",
        "\n",
        "print(\"The accuracy on train data is:\", round(train_accuracy_xgb, 3))\n",
        "print(\"The accuracy on test data is:\", round(accuracy_xgb, 3))\n",
        "print(\"The precision on test data is:\", round(precision_score_xgb, 3))\n",
        "print(\"The recall on test data is:\", round(recall_score_xgb, 3))\n",
        "print(\"The f1 on test data is:\", round(f1_score_xgb, 3))\n",
        "print(\"The roc_score on test data is:\", round(roc_score_xgb, 3))\n",
        "\n"
      ],
      "metadata": {
        "id": "55g6c74kAdmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XG Boosting\n",
        "cm_xgb_test = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"\\nConfusion Matrix for XG Boosting - Test:\")\n",
        "print(cm_xgb_test)\n"
      ],
      "metadata": {
        "id": "15QRzz_hAdjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SVM Classifier**"
      ],
      "metadata": {
        "id": "yp9Ipxk6URgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the SVM Classifier model\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Class prediction of y on test data\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "y_train_pred_svm = svm_classifier.predict(X_train)\n",
        "\n",
        "# Getting all scores for SVM Classifier on test data\n",
        "train_accuracy_svm = accuracy_score(y_train, y_train_pred_svm)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_score_svm = precision_score(y_test, y_pred_svm)\n",
        "recall_score_svm = recall_score(y_test, y_pred_svm)\n",
        "f1_score_svm = f1_score(y_test, y_pred_svm)\n",
        "roc_score_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "print(\"The accuracy on train data is:\", round(train_accuracy_svm, 3))\n",
        "print(\"The accuracy on test data is:\", round(accuracy_svm, 3))\n",
        "print(\"The precision on test data is:\", round(precision_score_svm, 3))\n",
        "print(\"The recall on test data is:\", round(recall_score_svm, 3))\n",
        "print(\"The f1 on test data is:\", round(f1_score_svm, 3))\n",
        "print(\"The roc_score on test data is:\", round(roc_score_svm, 3))\n"
      ],
      "metadata": {
        "id": "xPFUxom_AdXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "cm_svm_test = confusion_matrix(y_test, y_pred_svm)\n",
        "print(\"\\nConfusion Matrix for Logistic Regression - Test:\")\n",
        "print(cm_svm_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "qGWFttBbAdga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the models**"
      ],
      "metadata": {
        "id": "2VwYq2mCf_hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation metrics for each classifier\n",
        "classifiers = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Gradient Boosting', 'XG Boosting']\n",
        "train_accuracy = [train_accuracy_logi, train_accuracy_dt, train_accuracy_rf, train_accuracy_svm, train_accuracy_gb, train_accuracy_xgb]\n",
        "test_accuracy = [accuracy_logi, accuracy_dt, accuracy_rf, accuracy_svm, accuracy_gb, accuracy_xgb]\n",
        "precision_score = [precision_score_logi, precision_score_dt, precision_score_rf, precision_score_svm, precision_score_gb, precision_score_xgb]\n",
        "recall_score = [recall_score_logi, recall_score_dt, recall_score_rf, recall_score_svm, recall_score_gb, recall_score_xgb]\n",
        "f1_score = [f1_score_logi, f1_score_dt, f1_score_rf, f1_score_svm, f1_score_gb, f1_score_xgb]\n",
        "auc_score = [roc_score_logi, roc_score_dt, roc_score_rf, roc_score_svm, roc_score_gb, roc_score_xgb]\n"
      ],
      "metadata": {
        "id": "WRhXPk1KgEH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the results\n",
        "metrics =pd.DataFrame({\n",
        "    'Classifier': classifiers,\n",
        "    'Train Accuracy': train_accuracy,\n",
        "    'Test Accuracy': test_accuracy,\n",
        "    'Precision': precision_score,\n",
        "    'Recall': recall_score,\n",
        "    'F1 Score': f1_score,\n",
        "    'AUC': auc_score\n",
        "})\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Round off the values to 3 decimal places\n",
        "metrics_df = metrics_df.round(3)\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "RULxfpgNgEKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Create a list of model names and classifiers\n",
        "model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Gradient Boosting', 'XGBoost']\n",
        "classifiers = [logi, dt_classifier, rf_classifier, svm_classifier, gb_classifier, xgb_classifier]\n",
        "\n",
        "# Plot ROC AUC for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(model_names)):\n",
        "    y_pred_prob = classifiers[i].predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{model_names[i]} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC AUC Curve for Different Classifiers')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "67Cp1LJHgEN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Accuracy (coss-validation)"
      ],
      "metadata": {
        "id": "ytssZTH4vfdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}